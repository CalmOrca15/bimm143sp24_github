---
title: "lab08"
author: "Shreyas Sankaranarayanan"
format: pdf
editor: visual
---
##About

##Data Import
```{r}
# Save your input data file into your Project directory
fna.data <- "WisconsinCancer.csv"

# Complete the following code to input the data and store as wisc.df
wisc.df <- read.csv(fna.data, row.names=1)
wisc.data <- wisc.df[,-1]
diagnosis <- as.factor(wisc.df$diagnosis)
```

> Q1. How many observations are in this dataset?

According to the snippet below, there are 569 patients in this data set.

```{r}
nrow(wisc.data)

```


> Q2. How many observations have a malignant diagnosis?

According to the code snippet below, there are 212 patients with a malignant diagnosis

```{r}
sum(diagnosis == "M" )
table(diagnosis)
```


> Q3. How many variables/features in the data are suffixed with _mean?

There are 10 features with the suffix "_mean" according to the coding snippet below:

```{r}
sum(grepl("_mean" , names(wisc.data), fixed = TRUE))
grep("_mean", names(wisc.data))
```

##Initial Analysis

##PCA
> Q4.From your results, what proportion of the original variance is captured by the first principal components (PC1)?

> Q5.How many principal components (PCs) are required to describe at least 70% of the original variance in the data?

> Q6.How many principal components (PCs) are required to describe at least 90% of the original variance in the data?

##Clustering

We can try `kmeans()` clustering first:

```{r}
km <- kmeans(wisc.data, centers = 2)
table(km$cluster)

```
Cross-Table
```{r}
table(km$cluster, diagnosis)
```
Then try heirarchical clustering (`hclust()`): 

```{r}
hc <- hclust(dist(wisc.data))
plot(hc)
```
The question arises whether we need to scale so we must look at the standard deviations of our different data values
```{r}
round(apply(wisc.data,2,sd))
```
Yes, we need to scale and so we will run `prcomp()` with `scale=TRUE`

```{r}
wisc.pr <- prcomp(wisc.data, scale = TRUE)
summary(wisc.pr)
```

Generate our main PCA plot (score plot, PC1 v. PC2 plot)...

```{r}
library(ggplot2)
res <-as.data.frame(wisc.pr$x)
```

```{r}
ggplot(res, mapping = aes(PC1, PC2, col = diagnosis)) + geom_point()
```

Now we cluster on PCA results:

```{r}
d <- dist(wisc.pr$x[,1:7])
hcd<- hclust(d, method = "ward.D2")
plot(hcd)


```

```{r}
grps <- cutree(hcd, k=2)
table(grps)
```
To get clustering result/membership vector I need to cut the tree with `cutree()` function.

```{r}
grps <- cutree(hcd, k=2)
table(grps)
```

> Q. How many patients in each cluster group?

203 in Group 1 and 366 in group 2

```{r}
plot(res$PC1, res$PC2, col = grps)
```

#Prediction

We can utilzie PCA results (our model) to make predictions. We can take unseen data 

```{r}
url <- "https://tinyurl.com/new-samples-CSV"
new <- read.csv(url)
npc <- predict(wisc.pr, newdata=new)
npc
```

```{r}
plot(res$PC1, res$PC2, col = grps)
points(npc[,1],npc[,2], col = "blue", pch = 16, cex = 3)
text(npc[,1], npc[,2], labels = c(1,2), col = "white")
```

#Summary

Principal Component Analysis (PCA) is a super useful technique for analyzing large datasets. The algorithm finds new variables (PCs) that attempt to capture the maximum variance from original variables in the dataset.